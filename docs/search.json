[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, my name is Brody Pinto, and I’m a student at St. Lawrence University majoring in Data Science and minoring in Biology and Mathematics. This is my Data Visualization (DATA334) Blog where I will explore several different data sets and try to tell a visual data story with each. The goal of these blog posts is to create background for the data, minimize clutter to focus the reader’s attention on just a few important aspects of the data, and tell a story in a way that is engaging, intuitive, and insightful."
  },
  {
    "objectID": "posts/BlogPost2/index.html",
    "href": "posts/BlogPost2/index.html",
    "title": "Blog Post #2: Big Tech Stock Prices",
    "section": "",
    "text": "These data come from Yahoo Finance via Kaggle, compiled by Evan Gower. This data set consists of 8 variables involved in the daily stock prices and volumes of 14 of the largest tech companies and has 45,088 total observations. This data set includes the daily high and low stock prices of these 14 different tech companies (only recorded on weekdays) from the start of 2010 through the end of 2022. I found this data set from tidytuesday on GitHub from the following URL: https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-02-07/readme.md.\nQuestions of Interest\n\nHow have stock prices changed for these big tech companies over the past 12 years?\nWhich companies get the most yearly trading volume in the most recent year of the data set (2022)?\nFor the most popular company (Apple.com), can we make a model to accurately predict its stock prices?\n\nVariables of Interest\n\ncompany: the full name of the company\nstock_symbol: the stock symbol of the company\ndate: date of observation\nvolume: number of shares traded (buys and sells) for the day\nhigh: the highest market price for the day, in dollars per share\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(lubridate)\nlibrary(forecast)\nlibrary(openintro)\nlibrary(broom)\nlibrary(modelr)\nlibrary(here)\ntheme_set(theme_minimal())\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-02-07')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 6)\n\nbig_tech_stock_prices &lt;- tuesdata$big_tech_stock_prices\nbig_tech_companies &lt;- tuesdata$big_tech_companies\n\nstocks &lt;- big_tech_stock_prices |&gt; \n  left_join(big_tech_companies, by = c(\"stock_symbol\" = \"stock_symbol\")) |&gt;\n  relocate(company) |&gt;\n  mutate(weekday = weekdays(date),\n         month = month(date, label = TRUE),\n         year = year(date))"
  },
  {
    "objectID": "posts/BlogPost2/index.html#big-tech-stock-prices-data-set-introduction",
    "href": "posts/BlogPost2/index.html#big-tech-stock-prices-data-set-introduction",
    "title": "Blog Post #2: Big Tech Stock Prices",
    "section": "",
    "text": "These data come from Yahoo Finance via Kaggle, compiled by Evan Gower. This data set consists of 8 variables involved in the daily stock prices and volumes of 14 of the largest tech companies and has 45,088 total observations. This data set includes the daily high and low stock prices of these 14 different tech companies (only recorded on weekdays) from the start of 2010 through the end of 2022. I found this data set from tidytuesday on GitHub from the following URL: https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-02-07/readme.md.\nQuestions of Interest\n\nHow have stock prices changed for these big tech companies over the past 12 years?\nWhich companies get the most yearly trading volume in the most recent year of the data set (2022)?\nFor the most popular company (Apple.com), can we make a model to accurately predict its stock prices?\n\nVariables of Interest\n\ncompany: the full name of the company\nstock_symbol: the stock symbol of the company\ndate: date of observation\nvolume: number of shares traded (buys and sells) for the day\nhigh: the highest market price for the day, in dollars per share\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(lubridate)\nlibrary(forecast)\nlibrary(openintro)\nlibrary(broom)\nlibrary(modelr)\nlibrary(here)\ntheme_set(theme_minimal())\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-02-07')\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 6)\n\nbig_tech_stock_prices &lt;- tuesdata$big_tech_stock_prices\nbig_tech_companies &lt;- tuesdata$big_tech_companies\n\nstocks &lt;- big_tech_stock_prices |&gt; \n  left_join(big_tech_companies, by = c(\"stock_symbol\" = \"stock_symbol\")) |&gt;\n  relocate(company) |&gt;\n  mutate(weekday = weekdays(date),\n         month = month(date, label = TRUE),\n         year = year(date))"
  },
  {
    "objectID": "posts/BlogPost2/index.html#how-have-stock-prices-changed-for-these-big-tech-companies-over-the-past-12-years",
    "href": "posts/BlogPost2/index.html#how-have-stock-prices-changed-for-these-big-tech-companies-over-the-past-12-years",
    "title": "Blog Post #2: Big Tech Stock Prices",
    "section": "How have stock prices changed for these big tech companies over the past 12 years?",
    "text": "How have stock prices changed for these big tech companies over the past 12 years?\n\nyearly_price = stocks |&gt;\n  group_by(stock_symbol, year) |&gt;\n  summarise(mean_price = mean(high),\n            sd_price = sd(high),\n            sample_size = n()) |&gt;\n  mutate(se = sd_price / sqrt(sample_size), ## se for means\n         lower = mean_price - se,\n         upper = mean_price + se) |&gt;\n  left_join(big_tech_companies)\n  \nggplot(data = yearly_price, aes(x = year, y = mean_price, col = company)) +\n  geom_line(linewidth = 0.75) +\n  facet_wrap(~stock_symbol) +\n  theme(axis.text.x.bottom = element_text(size = 6)) +\n  labs(x = \"Year\", y = \"Mean Stock Price\", col = \"Company Name\")\n\n\n\n\nVisual Takeaways: For most of these big tech companies, there is generally an increase in mean stock price over time. This makes sense as tech is still considered a growing industry. Adobe and Netflix both have similar mean price trends: a steady increase in mean price until around 2020, immediately followed by a sharp drop. Most of these companies also see a decline in mean stock price after 2020. Tesla is also an interesting trend to point out: low and constant until 2019 when Tesla stock prices saw a very sharp increase up to almost $300 per share in 2020."
  },
  {
    "objectID": "posts/BlogPost2/index.html#which-companies-get-the-most-yearly-trading-volume-in-the-most-recent-year-of-the-data-set-2022",
    "href": "posts/BlogPost2/index.html#which-companies-get-the-most-yearly-trading-volume-in-the-most-recent-year-of-the-data-set-2022",
    "title": "Blog Post #2: Big Tech Stock Prices",
    "section": "Which companies get the most yearly trading volume in the most recent year of the data set (2022)?",
    "text": "Which companies get the most yearly trading volume in the most recent year of the data set (2022)?\n\nyearly_volume = stocks |&gt;\n  group_by(company, year) |&gt;\n  summarise(mean_volume = mean(volume),\n            sd_volume = sd(volume),\n            sample_size = n()) |&gt;\n  mutate(se = sd_volume / sqrt(sample_size), ## se for means\n         lower = mean_volume - se,\n         upper = mean_volume + se)\n\ntop_5_summary = yearly_volume |&gt; filter(year == 2022) |&gt;\n  arrange(desc(mean_volume)) |&gt;\n  filter(company == \"Apple Inc.\" || company == \"Tesla, Inc.\" || company == \"Amazon.com, Inc.\" || company == \"NVIDIA Corporation\" || company == \"Intel Corporation\") |&gt;\n  ungroup() |&gt;\n  mutate(company = fct_reorder(company, -mean_volume))\n\noptions(scipen=100)\nggplot(data = top_5_summary, aes(x = company, y = mean_volume)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), col = \"saddlebrown\", width = 0.1, linewidth = 0.75) +\n  geom_point(col = \"darkgreen\", size = 2) +\n  labs(x = \"Company\", y = \"Mean Daily Volume\")\n\n\n\n\nVisual Takeaways: Apple and Tesla are the most popular stocks to trade in 2022, with a mean daily trading volume of nearly 90 million. The error bars are \\(\\pm 1 se\\), and these illustrate the variability in the daily mean for the whole year of 2022. There is far less variance in volume for NVIDIA and Intel companies than Apple, Tesla, and Amazon. This means that there is less volatility in daily trading volume for NVIDIA and Intel Corporations."
  },
  {
    "objectID": "posts/BlogPost2/index.html#for-the-most-popular-company-apple.com-can-we-make-a-model-to-accurately-predict-its-stock-prices",
    "href": "posts/BlogPost2/index.html#for-the-most-popular-company-apple.com-can-we-make-a-model-to-accurately-predict-its-stock-prices",
    "title": "Blog Post #2: Big Tech Stock Prices",
    "section": "For the most popular company (Apple.com), can we make a model to accurately predict its stock prices?",
    "text": "For the most popular company (Apple.com), can we make a model to accurately predict its stock prices?\n\napple_stocks = stocks |&gt; filter(stock_symbol == \"AAPL\")\n\nmod_apple &lt;- lm(high ~ poly(date, degree = 6), data = apple_stocks) \n\ngrid &lt;- apple_stocks |&gt;\n  data_grid(\n    date = seq_range(date, n = 50))\n\naug_apple &lt;- augment(mod_apple, newdata = grid,\n                   interval = \"confidence\")\n\napple_stockprice = ggplot(data = apple_stocks, aes(x = date, y = high)) +\n  geom_line() +\n  geom_line(data = aug_apple, aes(x = date, y = .fitted),\n            colour = \"blue\", linewidth = 1.2) +\n  geom_ribbon(data = aug_apple, aes(y = .fitted,\n                                  ymin = .lower,\n                                  ymax = .upper), \n              alpha = 0.4) +\n  labs(x = \"Date\", y = \"Stock Price ($)\")\n\napple_stockprice\n\n\n\nglance(mod_apple)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.978         0.978  7.06    24570.       0     6 -11031. 22078. 22127.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nggsave(here(\"posts/BlogPost2/apple_stockprice.png\"), apple_stockprice)\n\nVisual Takeaways: The black curve is the observed stock prices for Apple.com and the blue curve is the the model used to predict Apple stock prices. Although it’s hard to see, there is also a shaded region around the blue curve that represents the 95% confidence interval for the model’s fitted values. The model seems to fit the actual trend in Apple stock price decently well (\\(R^2 = 0.978\\))."
  },
  {
    "objectID": "posts/BlogPost2/index.html#conclusion",
    "href": "posts/BlogPost2/index.html#conclusion",
    "title": "Blog Post #2: Big Tech Stock Prices",
    "section": "Conclusion",
    "text": "Conclusion\nThis data set was very interesting to explore, and I wish I had more time to delve into the data even deeper. There are certainly some flaws in just using a 6th degree polynomial to model the trend in Apple stock prices as there are certainly better methods that would be more appropriate to model this trend. Additionally, faceting the first graph makes it easier to visualize individual trends in stock prices, but makes it slightly harder to compare trends.\nIf I had more time on this data, I would definitely try to find a better modeling technique to better fit Apple stock prices.\nConnections to Class Ideas\n\nIn the second and third visualizations, I expressed the variability in the data using error bars and a shaded 95% interval region, respectively\nI also extended our linear modeling visualization work into visualizing a polynomial, I would like to also extend this method to certain pertinent statistical learning techniques"
  },
  {
    "objectID": "posts/BlogPost3/index.html",
    "href": "posts/BlogPost3/index.html",
    "title": "Blog Post #3: Bee Colonies",
    "section": "",
    "text": "These bee colony data come from the United States Department of Agriculture (USDA) website. This data set consists of 10 variables regarding bee colony health across the United States. This data set includes data from 2015 through 2021 (year-round data from 2015 through 2020) with a total of 1,222 observations. I found this data set from tidytuesday on GitHub from the following URL: https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-01-11/readme.md.\nQuestions of Interest\n\nWhich states have the most bee colonies in 2020?\nHow have those states been at renovating bee colonies from 2015 to 2020?\nHow have overall bee colony numbers in the United States changed between 2015 and 2020?\n\nVariables of Interest\n\nyear\nstate\ncolony_n: number of bee colonies\ncolony_reno_pct: percent of total colonies renovated (either by re-queening or nucleus colony)\ncolony_lost_pct: percent of total colonies lost\n\n\nlibrary(tidyverse)\nlibrary(maps)\nlibrary(here)\nlibrary(plotly)\n\ntheme_set(theme_minimal())\n\ncolony &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-11/colony.csv') |&gt; \n  mutate(season = if_else(months == \"April-June\" | months == \"July-September\",\n                          true = \"summer\",\n                          false = \"winter\"))"
  },
  {
    "objectID": "posts/BlogPost3/index.html#bee-colonies-data-set-introduction",
    "href": "posts/BlogPost3/index.html#bee-colonies-data-set-introduction",
    "title": "Blog Post #3: Bee Colonies",
    "section": "",
    "text": "These bee colony data come from the United States Department of Agriculture (USDA) website. This data set consists of 10 variables regarding bee colony health across the United States. This data set includes data from 2015 through 2021 (year-round data from 2015 through 2020) with a total of 1,222 observations. I found this data set from tidytuesday on GitHub from the following URL: https://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-01-11/readme.md.\nQuestions of Interest\n\nWhich states have the most bee colonies in 2020?\nHow have those states been at renovating bee colonies from 2015 to 2020?\nHow have overall bee colony numbers in the United States changed between 2015 and 2020?\n\nVariables of Interest\n\nyear\nstate\ncolony_n: number of bee colonies\ncolony_reno_pct: percent of total colonies renovated (either by re-queening or nucleus colony)\ncolony_lost_pct: percent of total colonies lost\n\n\nlibrary(tidyverse)\nlibrary(maps)\nlibrary(here)\nlibrary(plotly)\n\ntheme_set(theme_minimal())\n\ncolony &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-11/colony.csv') |&gt; \n  mutate(season = if_else(months == \"April-June\" | months == \"July-September\",\n                          true = \"summer\",\n                          false = \"winter\"))"
  },
  {
    "objectID": "posts/BlogPost3/index.html#which-states-have-the-most-bee-colonies-in-2020",
    "href": "posts/BlogPost3/index.html#which-states-have-the-most-bee-colonies-in-2020",
    "title": "Blog Post #3: Bee Colonies",
    "section": "Which states have the most bee colonies in 2020?",
    "text": "Which states have the most bee colonies in 2020?\n\n## find top 5 states with the most bee colonies in 2020:\ntop5_total = colony |&gt; filter(!is.na(colony_n) & year == 2020) |&gt;\n  group_by(state) |&gt;\n  summarise(average_colonies = mean(colony_n),\n            sd_colonies = sd(colony_n),\n            n_colonies = n()) |&gt;\n  mutate(se = sd_colonies / sqrt(n_colonies),\n         lower = average_colonies - se,\n         upper = average_colonies + se) |&gt;\n  arrange(desc(average_colonies)) |&gt;\n  ungroup() |&gt;\n  filter(state %in% c(\"California\", \"Florida\", \"North Dakota\", \"Texas\", \"Georgia\")) |&gt;\n  mutate(state = fct_reorder(state, desc(average_colonies)))\n\nggplot(data = top5_total, aes(x = state, y = average_colonies)) +\n  geom_col(colour = \"lightblue4\", fill = \"lightblue2\", position = \"dodge\") +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.15, linewidth = 0.5, color = \"black\") +\n  labs(x = \"State\", y = \"Average Total Bee Colonies\", caption = \"*Error bars are one standard error from the mean.\")\n\n\n\n\nVisual Takeaways: This plot shows the five states that are the most populous in bee colonies in 2020. California clearly has the most bee colonies for that year, also with the most variance from summer to winter months. I also want to point out that Georgia and Florida have relatively low variance throughout the year; this makes sense because their seasonal turnover is much less harsh than North Dakota and California."
  },
  {
    "objectID": "posts/BlogPost3/index.html#how-have-the-top-three-states-been-at-renovating-bee-colonies-from-2015-to-2020-both-in-summer-and-winter-months",
    "href": "posts/BlogPost3/index.html#how-have-the-top-three-states-been-at-renovating-bee-colonies-from-2015-to-2020-both-in-summer-and-winter-months",
    "title": "Blog Post #3: Bee Colonies",
    "section": "How have the top three states been at renovating bee colonies from 2015 to 2020, both in summer and winter months?",
    "text": "How have the top three states been at renovating bee colonies from 2015 to 2020, both in summer and winter months?\n\ntop3_yearly = colony |&gt; filter(state %in% c(\"California\", \"Florida\", \"North Dakota\") &\n                                 !is.na(colony_reno_pct)) |&gt;\n  group_by(year, state, season) |&gt;\n  summarise(average_renovation_pct = mean(colony_reno_pct),\n            sd_reno = sd(colony_reno_pct),\n            n_reno = n()) |&gt;\n  mutate(se_reno = sd_reno / sqrt(n_reno),\n         lower_reno = average_renovation_pct - se_reno,\n         upper_reno = average_renovation_pct + se_reno)\n\naverage_renovation_graph = ggplot(data = top3_yearly, aes(x = year, \n                                                          y = average_renovation_pct, \n                                                          col = state)) +\n  geom_line(linewidth = 1, alpha = 0.7) +\n  facet_wrap(~season) +\n  scale_color_viridis_d() +\n  labs(x = \"Year\", y = \"Average Renovation Percentage\", color = \"State\")\naverage_renovation_graph\n\n\n\n\n\nggsave(here(\"posts/BlogPost3/average_renovation_graph.png\"), average_renovation_graph)\n\nVisual Takeaways: The first trend I want to point out in this plot is that overall renovation percentage is much lower during winter months compared to summer months, even in places with mild winters like Florida. The difference appears to be much more drastic in the northern state of North Dakota where seasonal changes in climate are much greater. Additionally, there appears to be a peak in bee colony renovation in Georgia in 2016, followed by a constant decline in the following years. Conversely, California seems to be keeping up with their colony renovation, keeping their average renovation rate at around 18% during summer months and just over 5% during winter months."
  },
  {
    "objectID": "posts/BlogPost3/index.html#how-have-overall-bee-colony-numbers-in-the-united-states-changed-between-2015-and-2020",
    "href": "posts/BlogPost3/index.html#how-have-overall-bee-colony-numbers-in-the-united-states-changed-between-2015-and-2020",
    "title": "Blog Post #3: Bee Colonies",
    "section": "How have overall bee colony numbers in the United States changed between 2015 and 2020?",
    "text": "How have overall bee colony numbers in the United States changed between 2015 and 2020?\n\nstate_df &lt;- map_data(\"state\")\n\nstate_bees = colony |&gt;\n  mutate(state = tolower(state)) |&gt;\n  filter(year == 2015 | year == 2020) |&gt;\n  filter(!is.na(colony_lost_pct)) |&gt;\n  group_by(state, year) |&gt;\n  summarise(average_colonies_lost = mean(colony_lost_pct)) |&gt;\n  mutate(year = as_factor(year)) |&gt;\n  left_join(state_df, by = c(\"state\" = \"region\"), relationship = \"many-to-many\")\n\nmapplot = ggplot(data = state_bees,\n            mapping = aes(x = long, y = lat,\n                          group = group)) +\n  geom_polygon(colour = \"black\", aes(fill = average_colonies_lost,\n                                     label = average_colonies_lost,\n                                     label2 = state)) +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  theme_void() +\n  scale_fill_viridis_c(direction = -1) +\n  facet_wrap(~year) +\n  labs(fill = \"Average Colonies Lost (%)\")\n\nggplotly(mapplot, tooltip = c(\"label\", \"label2\"))\n\n\n\n\n\nVisual Takeaways: There appears to be an overall trend of improvement when it comes to average bee colonies lost, particularly across the eastern United States. The one state that doesn’t follow this general trend is Alabama where its average percent of colonies lost went from around 15% in 2015 to over 20% in 2020. On the other hand, the top five states from the first plot above (California, Florida, North Dakota, Texas, and Georgia) all saw a decrease in average bee colonies lost between 2015 and 2020.\nNote: This data set does not include bee colony data for the state of Nevada."
  },
  {
    "objectID": "posts/BlogPost3/index.html#conclusion",
    "href": "posts/BlogPost3/index.html#conclusion",
    "title": "Blog Post #3: Bee Colonies",
    "section": "Conclusion",
    "text": "Conclusion\nThis data set was particularly interesting to look at because I know that across the world, natural pollinators are on the decline on the global scale as a result of a loss of habitat with the ever-growing urbanization in our world. Bees also continue to be threatened by the invasive varroa mite and other diseases. This problem poses a concerning issue since we rely on natural pollinators, like bees, to pollinate our crops we use to feed ourselves and our animals. I found it somewhat surprising that in the United States in particular, there are efforts to renovate and support our bee populations. The final graph actually leaves us with a bit of hope that there does seem to be a positive trend of improvement when it comes to bee colony losses.\nThere may be some flaws to my approach in analyzing these data. I could have taken a different approach by looking at time series plots of bee colony numbers, renovation projects, and changes in colony losses over time. I am happy, however with the ways in which I presented the data. If I had more data (i.e. daily data), it would have been easier to construct a time series of the changes in bee colonies over time and try to make a predictive model for those changes.\nConnections to Class Ideas\n\nThese visuals are an effective way of communicating a story about bee colonies over space and time: I focused in on 5 of the states with the highest number of colonies and tried to share my findings in a way that is insightful.\nIn particular, in the first plot, I was able to connect the interpretations of the standard error bars to real-world ecological processes."
  },
  {
    "objectID": "posts/BlogPost1/index.html",
    "href": "posts/BlogPost1/index.html",
    "title": "Blog Post #1: US Tornado Data Exploration",
    "section": "",
    "text": "These data come from NOAA’s National Weather Service Storm Prediction Center. This data set consists of 27 different variables describing each observed tornado instance (68,693 total observations) in the United States from 1950 to 2022. I found this data set from tidytuesday on GitHub from the following URL: https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-05-16/readme.md.\nQuestions of Interest\n\nHow do total tornado occurrences vary within the United States - which state has seen the most tornadoes over the last 72 years?\nHow has the occurrence of extreme tornadoes (&gt;3F) changed over the last 72 years?\nIn which months are tornadoes most common?\n\nVariables of Interest\n\nyr: year of tornado occurrence\nmo: month of tornado occurrence\nst: two-letter abbreviation for the state\nmag: tornado magnitude on the Fujita Scale (0F-5F)\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(maps)\nlibrary(RColorBrewer)\n\ntornadoes &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-16/tornados.csv')\nhead(tornadoes)\n\n# A tibble: 6 × 27\n     om    yr    mo    dy date       time  tz    datetime_utc        st      stf\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;tim&gt; &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt; &lt;dbl&gt;\n1   192  1950    10     1 1950-10-01 21:00 Amer… 1950-10-02 03:00:00 OK       40\n2   193  1950    10     9 1950-10-09 02:15 Amer… 1950-10-09 08:15:00 NC       37\n3   195  1950    11    20 1950-11-20 02:20 Amer… 1950-11-20 08:20:00 KY       21\n4   196  1950    11    20 1950-11-20 04:00 Amer… 1950-11-20 10:00:00 KY       21\n5   197  1950    11    20 1950-11-20 07:30 Amer… 1950-11-20 13:30:00 MS       28\n6   194  1950    11     4 1950-11-04 17:00 Amer… 1950-11-04 23:00:00 PA       42\n# ℹ 17 more variables: mag &lt;dbl&gt;, inj &lt;dbl&gt;, fat &lt;dbl&gt;, loss &lt;dbl&gt;, slat &lt;dbl&gt;,\n#   slon &lt;dbl&gt;, elat &lt;dbl&gt;, elon &lt;dbl&gt;, len &lt;dbl&gt;, wid &lt;dbl&gt;, ns &lt;dbl&gt;,\n#   sn &lt;dbl&gt;, f1 &lt;dbl&gt;, f2 &lt;dbl&gt;, f3 &lt;dbl&gt;, f4 &lt;dbl&gt;, fc &lt;lgl&gt;"
  },
  {
    "objectID": "posts/BlogPost1/index.html#tornadoes-data-set-introduction",
    "href": "posts/BlogPost1/index.html#tornadoes-data-set-introduction",
    "title": "Blog Post #1: US Tornado Data Exploration",
    "section": "",
    "text": "These data come from NOAA’s National Weather Service Storm Prediction Center. This data set consists of 27 different variables describing each observed tornado instance (68,693 total observations) in the United States from 1950 to 2022. I found this data set from tidytuesday on GitHub from the following URL: https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-05-16/readme.md.\nQuestions of Interest\n\nHow do total tornado occurrences vary within the United States - which state has seen the most tornadoes over the last 72 years?\nHow has the occurrence of extreme tornadoes (&gt;3F) changed over the last 72 years?\nIn which months are tornadoes most common?\n\nVariables of Interest\n\nyr: year of tornado occurrence\nmo: month of tornado occurrence\nst: two-letter abbreviation for the state\nmag: tornado magnitude on the Fujita Scale (0F-5F)\n\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(maps)\nlibrary(RColorBrewer)\n\ntornadoes &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-16/tornados.csv')\nhead(tornadoes)\n\n# A tibble: 6 × 27\n     om    yr    mo    dy date       time  tz    datetime_utc        st      stf\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt;     &lt;tim&gt; &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt; &lt;dbl&gt;\n1   192  1950    10     1 1950-10-01 21:00 Amer… 1950-10-02 03:00:00 OK       40\n2   193  1950    10     9 1950-10-09 02:15 Amer… 1950-10-09 08:15:00 NC       37\n3   195  1950    11    20 1950-11-20 02:20 Amer… 1950-11-20 08:20:00 KY       21\n4   196  1950    11    20 1950-11-20 04:00 Amer… 1950-11-20 10:00:00 KY       21\n5   197  1950    11    20 1950-11-20 07:30 Amer… 1950-11-20 13:30:00 MS       28\n6   194  1950    11     4 1950-11-04 17:00 Amer… 1950-11-04 23:00:00 PA       42\n# ℹ 17 more variables: mag &lt;dbl&gt;, inj &lt;dbl&gt;, fat &lt;dbl&gt;, loss &lt;dbl&gt;, slat &lt;dbl&gt;,\n#   slon &lt;dbl&gt;, elat &lt;dbl&gt;, elon &lt;dbl&gt;, len &lt;dbl&gt;, wid &lt;dbl&gt;, ns &lt;dbl&gt;,\n#   sn &lt;dbl&gt;, f1 &lt;dbl&gt;, f2 &lt;dbl&gt;, f3 &lt;dbl&gt;, f4 &lt;dbl&gt;, fc &lt;lgl&gt;"
  },
  {
    "objectID": "posts/BlogPost1/index.html#how-do-total-tornado-occurrances-vary-within-the-united-states---which-state-has-seen-the-most-tornadoes-over-the-past-72-years",
    "href": "posts/BlogPost1/index.html#how-do-total-tornado-occurrances-vary-within-the-united-states---which-state-has-seen-the-most-tornadoes-over-the-past-72-years",
    "title": "Blog Post #1: US Tornado Data Exploration",
    "section": "How do total tornado occurrances vary within the United States - which state has seen the most tornadoes over the past 72 years?",
    "text": "How do total tornado occurrances vary within the United States - which state has seen the most tornadoes over the past 72 years?\n\nstate_df &lt;- map_data(\"state\")\n\nstate_abbrev = state_df |&gt; mutate(state = case_when(region == \"alabama\" ~ \"AL\",\n                                  region == \"alaska\" ~ \"AK\",\n                                  region == \"arizona\" ~ \"AZ\",\n                                  region == \"arkansas\" ~ \"AR\",\n                                  region == \"california\" ~ \"CA\",\n                                  region == \"colorado\" ~ \"CO\",\n                                  region == \"connecticut\" ~ \"CT\",\n                                  region == \"delaware\" ~ \"DE\",\n                                  region == \"florida\" ~ \"FL\",\n                                  region == \"georgia\" ~ \"GA\",\n                                  region == \"hawaii\" ~ \"HI\",\n                                  region == \"idaho\" ~ \"ID\",\n                                  region == \"illinois\" ~ \"IL\",\n                                  region == \"indiana\" ~ \"IN\",\n                                  region == \"iowa\" ~ \"IA\",\n                                  region == \"kansas\" ~ \"KS\",\n                                  region == \"kentucky\" ~ \"KY\",\n                                  region == \"louisiana\" ~ \"LA\",\n                                  region == \"maine\" ~ \"ME\",\n                                  region == \"maryland\" ~ \"MD\",\n                                  region == \"massachusetts\" ~ \"MA\",\n                                  region == \"michigan\" ~ \"MI\",\n                                  region == \"minnesota\" ~ \"MN\",\n                                  region == \"mississippi\" ~ \"MS\",\n                                  region == \"missouri\" ~ \"MO\",\n                                  region == \"montana\" ~ \"MT\",\n                                  region == \"nebraska\" ~ \"NE\",\n                                  region == \"nevada\" ~ \"NV\",\n                                  region == \"new hampshire\" ~ \"NH\",\n                                  region == \"new jersey\" ~ \"NJ\",\n                                  region == \"new mexico\" ~ \"NM\",\n                                  region == \"new york\" ~ \"NY\",\n                                  region == \"north carolina\" ~ \"NC\",\n                                  region == \"north dakota\" ~ \"ND\",\n                                  region == \"ohio\" ~ \"OH\",\n                                  region == \"oklahoma\" ~ \"OK\",\n                                  region == \"oregon\" ~ \"OR\",\n                                  region == \"pennsylvania\" ~ \"PA\",\n                                  region == \"rhode island\" ~ \"RI\",\n                                  region == \"south carolina\" ~ \"SC\",\n                                  region == \"south dakota\" ~ \"SD\",\n                                  region == \"tennessee\" ~ \"TN\",\n                                  region == \"texas\" ~ \"TX\",\n                                  region == \"utah\" ~ \"UT\",\n                                  region == \"vermont\" ~ \"VT\",\n                                  region == \"virginia\" ~ \"VA\",\n                                  region == \"washington\" ~ \"WA\",\n                                  region == \"west virginia\" ~ \"WV\",\n                                  region == \"wisconsin\" ~ \"WI\",\n                                  region == \"wyoming\" ~ \"WY\",\n                                  TRUE ~ NA_character_))\n\ntornadoes_summary = tornadoes |&gt; group_by(st) |&gt;\n  summarise(total_tornadoes = n(),\n            mean_magnitude = mean(mag),\n            mean_injuries = mean(inj),\n            mean_fatalities = mean(fat),\n            mean_property_loss = mean(loss),\n            mean_length = mean(len),\n            mean_width = mean(wid))\n\nstate_tornadoes = left_join(state_abbrev, tornadoes_summary, by = c(\"state\" = \"st\"))\n\nmap_totaltornadoes = ggplot(data = state_tornadoes,\n            mapping = aes(x = long, y = lat,\n                          group = group)) +\n  geom_polygon(colour = \"black\", aes(fill = total_tornadoes)) +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  theme_void() +\n  scale_fill_distiller(palette = \"Reds\", direction = 0.5)\n\nmap_totaltornadoes\n\n\n\n\n\nggsave(here(\"posts/BlogPost1/total_tornadoes_map.png\"), map_totaltornadoes)\n\nVisual Takeaways: This is a map of the United States, colored by total tornado instances over the past 72 years. Texas stands out as having a significantly higher number of tornadoes than the rest of the country. This may be surprising, as one might expect either Kansas or Oklahoma to take the top spot (while those two states appear to be coming in the second and third most tornadoes). One possible explanation is that Texas is significantly larger in terms of pure land area. One visible trend is “tornado alley” through the center of the United States from Texas through South Dakota (including eastern Colorado). Florida is also an interesting case, having a similar number of tornadoes to states in tornado alley. This makes sense because tornadoes often occur during other severe storm events such as thunderstorms, tropical storms, and hurricanes."
  },
  {
    "objectID": "posts/BlogPost1/index.html#how-has-the-occurrence-of-extreme-tornadoes-3f-changed-over-the-last-72-years",
    "href": "posts/BlogPost1/index.html#how-has-the-occurrence-of-extreme-tornadoes-3f-changed-over-the-last-72-years",
    "title": "Blog Post #1: US Tornado Data Exploration",
    "section": "How has the occurrence of extreme tornadoes (>3F) changed over the last 72 years?",
    "text": "How has the occurrence of extreme tornadoes (&gt;3F) changed over the last 72 years?\n\ntornadoes_mag_sum = tornadoes |&gt; filter(mag &gt; 2) |&gt;\n  group_by(yr,mag) |&gt;\n  summarise(tornado_occurrences = n()) |&gt;\n  mutate(mag = as_factor(mag))\n\nggplot(data = tornadoes_mag_sum, aes(x = yr, y = tornado_occurrences, col = mag)) +\n  geom_line(linewidth = 0.75) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Year\", y = \"Severe Tornado Occurences\") +\n  theme_minimal()\n\n\n\n\nVisual Takeaways: There doesn’t appear to be any strong trend in extreme tornadoes in the United States. The trend for severe (3F: 158-206 mph), devastating (4F: 207-260 mph), and incredible (5F: 261-318 mph) tornadoes seems to be random and relatively unpredictable. With increasing instability in the environment due to global climate change, I expected to see an increase in more intense tornadoes from 1950 to 2022. For severe tornadoes, however, there has been a slight overall decrease in occurrences over time."
  },
  {
    "objectID": "posts/BlogPost1/index.html#in-which-months-are-tornadoes-most-common",
    "href": "posts/BlogPost1/index.html#in-which-months-are-tornadoes-most-common",
    "title": "Blog Post #1: US Tornado Data Exploration",
    "section": "In which months are tornadoes most common?",
    "text": "In which months are tornadoes most common?\n\nmonth_tornado = tornadoes |&gt;\n  group_by(mo) |&gt;\n  summarise(Month_totals = n()) |&gt;\n  mutate(Month = case_when(mo == 1 ~ \"Jan\",\n                           mo == 2 ~ \"Feb\",\n                           mo == 3 ~ \"Mar\",\n                           mo == 4 ~ \"Apr\",\n                           mo == 5 ~ \"May\",\n                           mo == 6 ~ \"Jun\",\n                           mo == 7 ~ \"Jul\",\n                           mo == 8 ~ \"Aug\",\n                           mo == 9 ~ \"Sep\",\n                           mo == 10 ~ \"Oct\",\n                           mo == 11 ~ \"Nov\",\n                           mo == 12 ~ \"Dec\")) |&gt;\n  mutate(Month = as.factor(x = Month),\n         Month = fct_reorder(Month, mo))\n\ntop_3 = month_tornado |&gt;\n  filter(Month %in% c(\"Apr\", \"May\", \"Jun\"))\n\nggplot(data = month_tornado, aes(x = Month, y = Month_totals)) +\n  geom_segment(aes(xend = Month, y = 0, yend = Month_totals)) +\n  geom_point() +\n  geom_segment(data = top_3, aes(xend = Month, y = 0, yend = Month_totals), col = \"darkred\", linewidth = 2) +\n  geom_point(data = top_3, col = \"darkred\", size = 3) +\n  labs(y = \"Tornado Occurrences\") +\n  theme_minimal()\n\n\n\n\nVisual Takeaway: The most tornadoes occur during the start of summer - particularly April, May, and June. These three months seem significantly higher than the rest, which makes sense due to the typical atmospheric conditions that exist during this time. The introduction of more warm, moist air, combined with cooler, dry air causes instability in the lower atmosphere. The dryness of the winter months lead to lower instances of tornadoes in the United States."
  },
  {
    "objectID": "posts/BlogPost1/index.html#conclusion",
    "href": "posts/BlogPost1/index.html#conclusion",
    "title": "Blog Post #1: US Tornado Data Exploration",
    "section": "Conclusion",
    "text": "Conclusion\nIn particular for the map of total tornadoes, I could standardize the values with respect to tornadoes per state land area so that the massive area of Texas doesn’t stand out as much in the visualization. If I did it this way, I would likely see a change in colors in the map with likely higher values in states like Oklahoma and Kansas. In terms of the line plot for extreme tornadoes over time, the plot is a bit underwhelming. This is likely because of the lack of striking trend that we see in the other two figures.\nIf I had more time to work with these data, I would try to explore relationships involving tornado size (length and width) and magnitude with property losses and injuries/fatalities. I think it would be interesting to see if I could make a model that predicts either property loss or injuries/fatalities using tornado size and magnitude as well as time of day. The datetime_utc variable could also be used to view the data as a time series.\nConnection to Class Ideas\nThese visuals effectively communicate information about tornado data in the United States by incorporating color, cleanliness, and organization to appeal to the reader’s visual senses. In terms of human perception, changes in color are much easier to distinguish than changes in shape, so I made sure to use color that grabs the reader’s attention whenever applicable. For the map of the United States, I used a sequential, continuous color scale to represent the total number of tornado occurrences per state. This color scale is ideal for representing this type of data because it allows for a quick, effective interpretation of the intended results. Finally, I tried to maximize data-to-ink ratio as much as possible to ensure the reader’s attention was in the right place and not distracted by any superfluous detail."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Data Visualization Blog - Brody Pinto",
    "section": "",
    "text": "This is my Data Visualization Blog. Here, I will explore a few different data sets and attempt to tell a story for each of them. My goal is to follow the processes involved with “data storytelling”:\n\nunderstand and explain the context of the data\nchoose effective visuals\neliminate clutter\nfocus attention on a particular trend(s) in the data\ntell a story!"
  },
  {
    "objectID": "posts/BlogPost4/index.html",
    "href": "posts/BlogPost4/index.html",
    "title": "Blog Post #3: Data Science Salaries Final Project",
    "section": "",
    "text": "For this final project, I made a Shiny App to look at job salaries in the field of data science and other related fields. This project is of an exploratory nature, thus there is no one driving research question I aimed to answer. Instead, my goal was to provide an app that allows the user to visualize any trends they may be interested in, giving the user the autonomy to explore the data in any way they choose. This high degree of interactivity, however, makes the process of “data storytelling” nearly impossible. However, in this report, I will highlight three questions of interest with the hopes of providing insight towards the use and interpretation of the app. The case studies presented in this report aim to examine the distribution of pay for data scientists starting their careers, to visualize the increasing trend in data analytics jobs over time, and to emphasize the spread in salaries for engineering positions of different specialties."
  },
  {
    "objectID": "posts/BlogPost4/index.html#abstract",
    "href": "posts/BlogPost4/index.html#abstract",
    "title": "Blog Post #3: Data Science Salaries Final Project",
    "section": "",
    "text": "For this final project, I made a Shiny App to look at job salaries in the field of data science and other related fields. This project is of an exploratory nature, thus there is no one driving research question I aimed to answer. Instead, my goal was to provide an app that allows the user to visualize any trends they may be interested in, giving the user the autonomy to explore the data in any way they choose. This high degree of interactivity, however, makes the process of “data storytelling” nearly impossible. However, in this report, I will highlight three questions of interest with the hopes of providing insight towards the use and interpretation of the app. The case studies presented in this report aim to examine the distribution of pay for data scientists starting their careers, to visualize the increasing trend in data analytics jobs over time, and to emphasize the spread in salaries for engineering positions of different specialties."
  },
  {
    "objectID": "posts/BlogPost4/index.html#introduction",
    "href": "posts/BlogPost4/index.html#introduction",
    "title": "Blog Post #3: Data Science Salaries Final Project",
    "section": "Introduction",
    "text": "Introduction\nThe dataset I used in this project is called Latest Data Science Job Salaries, published on Kaggle by Sourav Banerjee: https://www.kaggle.com/datasets/iamsouravbanerjee/data-science-salaries-2023. These data encompass job salary data for the field of data science and other related fields through the years 2020 through early 2024. Most of the salary data comes from the United States of America, but also does include data from 74 other countries across the world. This dataset has been instrumental in my understanding of how the field of data science and its associated compensations have changed over recent years.\nVariables of Interest\n\nSalary.in.USD: job salary converted to USD\nJob.Title: job position title\nJob.Category: category of job position (added by myself)\nExperience.Level: factor of individual’s experience level (Entry, Mid, Senior, Executive)\nCompany.Size: factor of the company’s size (Small, Medium, Large)\nCompany.Location: geographic location of the company\nEmployment.Type: factor of job position type (Full-Time, Part-Time, Freelance)\nYear: year of employment (2020-2024)\n\nQuestions of Interest\n\nWhat is the distribution of salary for Data Scientist Entry- and Mid-level positions?\nHow has the experience level and pay of Data Analysts changed over the course of this dataset (2020 through early 2024)?\nFor the Engineer position, how does the shape of the pay differ for different specialties (AI Engineer vs. Data Science Engineer vs. Machine Learning Engineer)?"
  },
  {
    "objectID": "posts/BlogPost4/index.html#case-study-visuals",
    "href": "posts/BlogPost4/index.html#case-study-visuals",
    "title": "Blog Post #3: Data Science Salaries Final Project",
    "section": "Case Study Visuals",
    "text": "Case Study Visuals\n\n## This is the code to set up the static graphs adapted from my Shiny App\n#/ warning: false\n#/ output: false\nlibrary(tidyverse)\n\noptions(scipen = 100)\n\ntheme_set(theme_minimal())\n\nsalary &lt;- utils::read.csv(here::here(\"data/v7_Latest_Data_Science_Salaries.csv\"), stringsAsFactors = TRUE) |&gt;\n  filter(!is.na(Salary.in.USD))\n\nsalary = salary |&gt;\n  mutate(Job.Category = case_when(str_detect(Job.Title, pattern = \"AI \") == TRUE ~ \"Artificial Intelligence\",\n                                  str_detect(Job.Title, pattern = \"Autonomous\") == TRUE ~ \"Artificial Intelligence\",\n                                  str_detect(Job.Title, pattern = \"Computer Vision\") == TRUE ~ \"Artificial Intelligence\",\n                                  str_detect(Job.Title, pattern = \"NLP\") == TRUE ~ \"Artificial Intelligence\",\n                                  str_detect(Job.Title, pattern = \"Prompt Engineer\") == TRUE ~ \"Artificial Intelligence\",\n                                  str_detect(Job.Title, pattern = \"BI \") == TRUE ~ \"Business Intelligence\",\n                                  str_detect(Job.Title, pattern = \"Business\") == TRUE ~ \"Business Intelligence\",\n                                  str_detect(Job.Title, pattern = \"Data Product\") == TRUE ~ \"Business Intelligence\",\n                                  str_detect(Job.Title, pattern = \"Financ\") == TRUE ~ \"Business Intelligence\",\n                                  str_detect(Job.Title, pattern = \"Data Sci\") == TRUE ~ \"Data Science\",\n                                  str_detect(Job.Title, pattern = \"Data Lead\") == TRUE ~ \"Data Science\",\n                                  str_detect(Job.Title, pattern = \"Data Manage\") == TRUE ~ \"Data Science\",\n                                  str_detect(Job.Title, pattern = \"Data Specialist\") == TRUE ~ \"Data Science\",\n                                  str_detect(Job.Title, pattern = \"Head of Data\") == TRUE ~ \"Data Science\",\n                                  str_detect(Job.Title, pattern = \"ML\") == TRUE ~ \"Machine Learning\",\n                                  str_detect(Job.Title, pattern = \"Machine\") == TRUE ~ \"Machine Learning\",\n                                  str_detect(Job.Title, pattern = \"Deep Learning\") == TRUE ~ \"Machine Learning\",\n                                  str_detect(Job.Title, pattern = \"Big \") == TRUE ~ \"Big Data\",\n                                  str_detect(Job.Title, pattern = \"Cloud\") == TRUE ~ \"Cloud Data\",\n                                  str_detect(Job.Title, pattern = \"Data Architect\") == TRUE ~ \"Data Organization\",\n                                  str_detect(Job.Title, pattern = \"Data Engineer\") == TRUE ~ \"Data Organization\",\n                                  str_detect(Job.Title, pattern = \"Data Developer\") == TRUE ~ \"Data Organization\",\n                                  str_detect(Job.Title, pattern = \"Data Infrastructure\") == TRUE ~ \"Data Organization\",\n                                  str_detect(Job.Title, pattern = \"ETL \") == TRUE ~ \"Data Organization\",\n                                  str_detect(Job.Title, pattern = \"Data Operations\") == TRUE ~ \"Data Operations\",\n                                  str_detect(Job.Title, pattern = \"DevOps\") == TRUE ~ \"Data Operations\",\n                                  str_detect(Job.Title, pattern = \"Data Integration\") == TRUE ~ \"Data Operations\",\n                                  str_detect(Job.Title, pattern = \"Data Strateg\") == TRUE ~ \"Data Operations\",\n                                  str_detect(Job.Title, pattern = \"Decision Scientist\") == TRUE ~ \"Data Operations\",\n                                  str_detect(Job.Title, pattern = \"Data Model\") == TRUE ~ \"Data Modeling\",\n                                  str_detect(Job.Title, pattern = \"Data Quality\") == TRUE ~ \"Data Quality\",\n                                  str_detect(Job.Title, pattern = \"Data Vis\") == TRUE ~ \"Data Visualization\",\n                                  str_detect(Job.Title, pattern = \"Research Analyst\") == TRUE ~ \"Research\",\n                                  str_detect(Job.Title, pattern = \"Research Engineer\") == TRUE ~ \"Research\",\n                                  str_detect(Job.Title, pattern = \"Research Scientist\") == TRUE ~ \"Research\",\n                                  str_detect(Job.Title, pattern = \"Applied Scientist\") == TRUE ~ \"Research\",\n                                  str_detect(Job.Title, pattern = \"Analy\") == TRUE ~ \"Data Analytics\")) |&gt;\n  mutate(Company.Location = as.character(Company.Location),\n         Employment.Type = as.character(Employment.Type),\n         Experience.Level = fct_relevel(Experience.Level, c(\"Entry\", \"Mid\", \"Senior\", \"Executive\")),\n         Company.Size = fct_relevel(Company.Size, c(\"Small\", \"Medium\", \"Large\")))\n\n1. What is the distribution of salary for Data Scientist Entry- and Mid-level positions?\n\nds_static = salary |&gt; \n  filter(Job.Title == \"Data Scientist\") |&gt;\n  filter(Experience.Level %in% c(\"Entry\", \"Mid\"))\nggplot(data = ds_static, aes(x = Salary.in.USD)) +\n      geom_histogram(bins = 15, col = \"lightblue4\", fill = \"lightblue\") +\n      labs(title = glue::glue(\"Entry and Mid Level Data Scientist Salaries\")) +\n      theme_minimal(base_size = 18) +\n      labs(x = \"Salary (USD)\")\n\n\n\n\nThis histogram of data scientist salaries shows the right-skewed trend of salaries for entry and mid level positions. You can tell that the majority of the jobs offer a yearly salary anywhere between roughly $80,000 and $120,000 for data scientists on the lower side of the experience spectrum. There are a few outliers to the right of the graph, thus causing the right skewness: there are a couple indivials getting paid over $300,000!\n2. How has the experience level and pay of Data Analysts changed over the course of this dataset (2020 through early 2024)?\n\nda_static = salary |&gt;\n      filter(Job.Title == \"Data Analyst\")\nggplot(data = da_static, aes(x = Salary.in.USD, color = Experience.Level)) +\n      geom_freqpoly(linewidth = 1) +\n      facet_wrap(~ Year) +\n      labs(title = glue::glue(\"Salary for Data Analyst Jobs\")) +\n      theme_minimal(base_size = 12) +\n      labs(x = \"Salary (USD)\", color = \"Experience Level\") +\n      scale_color_viridis_d() +\n      theme(axis.text.x = element_text(angle = -45))\n\n\n\n\nThis faceted frequency plot can be used to visualize the trend in data analysts over the past 4 years. There is not much data in 2020 and 2021, but you can still see a small increase in the number of data analysts between these two years; note there is an introduction of senior level positions in the year 2021. The increase in the number of data analysts (for all four experience levels) in 2022 is great, and this trend is magnified in 2023. It is interesting to note that the shape of senior level data analysts in 2022 and 2023 is essentially normally distributed with a clear center salary that is visually higher than the other three experience levels in the visual.\n3. For the Engineer position, how does the shape of the pay differ for different specialties (AI Engineer vs. Data Science Engineer vs. Machine Learning Engineer)?\n\nviolin_df = salary |&gt;\n      filter(Job.Title %in% c(\"AI Engineer\", \"Data Science Engineer\", \"Machine Learning Engineer\"))\nggplot(data = violin_df, aes(x = Job.Title, y = Salary.in.USD)) +\n      geom_violin(fill = \"lightgreen\", color = \"forestgreen\") +\n      labs(x = \"Job Title\", y = \"Salary (USD)\") +\n      theme_minimal(base_size = 18) +\n      theme(axis.text.x = element_text(angle = -10))\n\n\n\n\nThe centers of these three engineering specialties are all roughly comparable, sitting at just under a $200,000 yearly salary. However, the range of salaries for AI Engineers and Machine Learning Engineers appears to be significantly wider than the range of Data Science Engineers: AI Engineers and Machine Learning Engineers are making up to about $380,000 (note there is an outlier where a Machine Learning Engineer is making about $750,000) while Data Science Engineers are only making up to $280,000. That’s a whole $100,000 less! Another visual aspect to take away here is the shape of the data. The shape of Data Science Engineers and Machine Learning Engineers (ignoring the outlier) appear to be almost perfectly normally distributed while there may be a small issue with right skewness."
  },
  {
    "objectID": "posts/BlogPost4/index.html#conclusions",
    "href": "posts/BlogPost4/index.html#conclusions",
    "title": "Blog Post #3: Data Science Salaries Final Project",
    "section": "Conclusions",
    "text": "Conclusions\nThese three visuals hopefully provide some guidance and insight into how my Shiny App can be used to visualize different patterns in the data. The three questions of interest in this short report are just a glimpse into the world of questions that can be answered with data like this.\nAs far as limitations go, this dataset is relatively small and not comprehensive at all. If I had more time, I could search for more comprehensive datasets to conduct more research on. I would find it particularly interesting to visualize trends in salaries on a longer time frame (i.e., to look at COVID effects on job salaries). Another direction I could have taken these data is a logistic regression approach to try and predict experience based on salary, for example.\n\nLink to the Shiny App: https://brodypinto.shinyapps.io/DATA334_final_project/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DATA334blog",
    "section": "",
    "text": "Blog Post #3: Data Science Salaries Final Project\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2024\n\n\nBrody Pinto\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post #3: Bee Colonies\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2024\n\n\nBrody Pinto\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post #2: Big Tech Stock Prices\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nBrody Pinto\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post #1: US Tornado Data Exploration\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nBrody Pinto\n\n\n\n\n\n\n  \n\n\n\n\nData Visualization Blog - Brody Pinto\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nBrody Pinto\n\n\n\n\n\n\nNo matching items"
  }
]